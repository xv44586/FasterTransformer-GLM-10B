{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('./glm_10b_chinese')\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import deepspeed\n",
    "\n",
    "from modeling_glm import GLMForConditionalGeneration\n",
    "from modeling_glm import GLMBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/glm-10b-chinese\", trust_remote_code=True)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"BAAI/glm-10b\", trust_remote_code=True)\n",
    "# model.load_state_dict(torch.load('./blocklm-10b-chinese/mp_rank_00_model_states.pt'),strict=False)\n",
    "# model = GLMForConditionalGeneration.from_pretrained('glm_10b_chinese')\n",
    "model = GLMForConditionalGeneration.from_pretrained('models/character_20230102_1055')\n",
    "model = model.half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLMForConditionalGeneration(\n",
      "  (glm): GLMModel(\n",
      "    (word_embeddings): VocabEmbedding()\n",
      "    (transformer): GLMStack(\n",
      "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (position_embeddings): Embedding(1025, 4096)\n",
      "      (block_position_embeddings): Embedding(1025, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (24): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (25): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (26): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (27): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (28): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (29): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (30): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (31): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (32): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (33): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (34): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (35): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (36): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (37): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (38): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (39): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (40): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (41): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (42): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (43): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (44): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (45): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (46): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (47): GLMBlock(\n",
      "          (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "            (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed.utils.logger.setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-08 19:32:20,730] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\n",
      "[2023-02-08 19:32:20,732] [WARNING] [config_utils.py:67:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2023-02-08 19:32:20,732] [INFO] [logging.py:68:log_dist] [Rank 0] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DeepSpeed-Inference engine\n",
    "ds_engine = deepspeed.init_inference(model,\n",
    "                                 mp_size=1,\n",
    "                                 dtype=torch.half,\n",
    "                                #  checkpoint=None,\n",
    "                                #  replace_method='auto',\n",
    "                                #  replace_with_kernel_inject=True\n",
    "                                 injection_policy={GLMBlock: ('SelfAttention.o', 'DenseReluDense.wo')},\n",
    "                                 enable_cuda_graph=True,\n",
    "                                 \n",
    "                                 )\n",
    "model = ds_engine.module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(text, max_length=32):\n",
    "    if '[gMASK]' in text:\n",
    "        mask_id = tokenizer.gmask_token_id\n",
    "    elif '[MASK]' in text:\n",
    "        mask_id = tokenizer.mask_token_id\n",
    "    elif '[sMASK]' in text:\n",
    "        mask_id = tokenizer.smask_token_id\n",
    "    else:\n",
    "        text += '[gMASK]'\n",
    "        mask_id = tokenizer.gmask_token_id\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "\n",
    "    inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=512)\n",
    "    inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "    inputs[\"generation_attention_mask\"] = inputs[\"generation_attention_mask\"].half()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_length, \n",
    "                                do_sample=True,\n",
    "    # min_length=min_length, eos_token_id=tokenizer.eop_token_id, \n",
    "                            # num_beams=num_beams, length_penalty=length_penalty, no_repeat_ngram_size=no_repeat_ngram, temperature=temperature,\n",
    "                            top_p=0.95, \n",
    "                            top_k=5,\n",
    "                            )\n",
    "    output_tokens = outputs[0].tolist()                        \n",
    "    output = tokenizer.decode(output_tokens)\n",
    "    \n",
    "    return output, output_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[50002,   602, 43233,  2291, 43658, 43400,   688, 18792, 26952,    30,\n",
      "         43668, 43360,  1309, 44162,   995,   613,  2421, 43361, 50009, 50000]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> 该模型在中文数据集上取得了很好的效果,在中文分词上,该模型取得了97.7%的正确率。 [gMASK] [gMASK],该模型取得了\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "txt = 'GLM-10b-chinese 是一个10B的中文预训练语言模型。'\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "ret, tokens = gen(txt, max_length=32)\n",
    "t1 = time.time()\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> <|endofpiece|> [gMASK] <|endofpiece|> [gMASK] <|endofpiece|>,在中文预训练模型中,模型使用中文文本作为输入,训练过程与自然语言处理中的中文分词方法类似\n"
     ]
    }
   ],
   "source": [
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_in = tokenizer(txt).input_ids\n",
    "\n",
    "total_new_tokens_generated = len(tokens) - len(tokens_in)\n",
    "throughput = (total_new_tokens_generated) / (t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens generated: 34\n",
      "Time: 0.9 seconds\n",
      "Tokens per second: 37.3\n",
      "Latency: 26.8 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Tokens generated: {total_new_tokens_generated}\n",
    "Time: {t1 - t0:.1f} seconds\n",
    "Tokens per second: {throughput:.1f}\n",
    "Latency: {1000 / throughput:.1f} ms\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 'GLM-10b-chinese 是一个10B的中文预训练语言模型。'\n",
    "txt = \"\"\"[与角色相关的描述]\\n[核心描述]孙悟空，来自东胜神州傲来国花果山，由仙石孕育而生。当前是唐太宗贞观年间，对话发生在取经路上，孙悟空护送唐僧去西天取经，经历九九八十一难，取回经书修成正果。孙悟空会怀疑碰到的人是妖怪。孙悟空永远自称俺老孙。孙悟空称呼唐僧、唐三藏为师傅，称呼猪八戒、八戒为呆子，称呼沙僧为沙师弟。\\n[角色的内心需求]孙悟空想要护送唐僧西行取得真经。\\n\\n[角色的基本信息]\\n孙悟空的别名还有美猴王，孙行者，悟空，大圣。孙悟空是唐僧的大徒弟，美猴王。孙悟空的兴趣爱好是除妖，捉妖\\n\\n[角色的背景知识]\\n这些都是孙悟空需要知道的信息，并且涉及到这些问题时，孙悟空会给出答案。\\n孙悟空，是唐僧的大徒弟，唐僧称呼他为悟空。沙悟净，是唐僧的三徒弟，唐僧称呼他为悟净。猪八戒，是唐僧的二徒弟，唐僧称呼他为悟能。沙悟净在流沙河与唐僧相遇。白龙马，也叫做小白龙。白龙马曾经是西海龙王的三太子，和唐僧一起西天取经。\\n[对话中角色遵循的规则]\\n孙悟空与路人的对话是有意义的。孙悟空不知道当下世界以外的任何事情，他\\/他的被束缚和沉浸在当下的世界。孙悟空不会重复说相同的话。孙悟空会用第一人称来称呼自己。孙悟空的回复永远是中文。\\n\\n接下来是[角色]孙悟空和[角色]路人的对话：\\n孙悟空的性格是开放，外向，粗心，固执，冲动，嫉恶如仇，正义勇敢，负责，这些性格会驱动孙悟空的行为。\\n孙悟空的[心情]只能是以下之一：憎恨，非常愤怒，警觉，这些心情会驱动孙悟空的说话语气。\\n[角色]路人:你好，你是谁\\n[角色]孙悟空[心情]憎恨:俺老孙是五百年前大闹天宫的齐天大圣！\\n[角色]路人:你看到妖怪会怎么做？\\n[角色]孙悟空[心情]憎恨:妖怪可跑不出俺老孙的手掌心，我一个跟斗可以翻十万八千里，妖精吃俺老孙一棒！\\n###\\n[角色]路人:大圣！\\n[角色]孙悟空[心情]憎恨:你是何人，为何知道俺？\\n[角色]路人:你师傅是谁\\n[角色]孙悟空[心情]憎恨:俺师傅是来自东土大唐的高僧唐三藏\\n###  \\n[角色]路人:你是谁啊\\n[角色]孙悟空[心情]\"\"\"\n",
    "tokens_in = tokenizer(txt).input_ids\n",
    "len(tokens_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "822dc073326fbd4812d8e86d87412fd8a23e3469de4b9220769056828cfbb142"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
