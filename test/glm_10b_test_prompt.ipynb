{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要思路：\n",
    "1.在输入时，拼接prompt_tokens,构造完整的inputs_ids,postion_ids,attention_mask\n",
    "2.第一次生成前，prepare_for_generate 时，将对应的prompt_cache 更新进past，同时将input_ids/position_ids/attention_mask 进行调整，恢复到正常“input”的状态 \n",
    "3.后续的不用修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('./glm_10b_chinese')\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import deepspeed\n",
    "\n",
    "from modeling_glm_cache_pruning_prompt import GLMForConditionalGeneration\n",
    "# from modeling_glm_cache import GLMBlock\n",
    "\n",
    "# from modeling_glm import GLMForConditionalGeneration\n",
    "from tokenization_glm import GLMChineseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GLMChineseTokenizer.from_pretrained('./glm_10b_chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"BAAI/glm-10b-chinese\", trust_remote_code=True)\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"BAAI/glm-10b\", trust_remote_code=True)\n",
    "# model.load_state_dict(torch.load('./blocklm-10b-chinese/mp_rank_00_model_states.pt'),strict=False)\n",
    "# model = GLMForConditionalGeneration.from_pretrained('glm_10b_chinese')\n",
    "model = GLMForConditionalGeneration.from_pretrained('models/character_20230102_1055')\n",
    "model = model.half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)\n",
    "def post_process_cache_for_beamsearch(p_cache, num_beams):\n",
    "    if num_beams and num_beams > 1:\n",
    "        p_cache = tuple(\n",
    "            tuple(past_state.expand(num_beams, -1, -1, -1) for past_state in layer_past)\n",
    "            for layer_past in p_cache\n",
    "            )\n",
    "    return p_cache        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(text, max_length=32, num_beams=None, prompt_text='',prompt_cache=None):\n",
    "    if '[gMASK]' not in text:\n",
    "        text += '[gMASK]'\n",
    "\n",
    "    if prompt_text:\n",
    "        text = prompt_text + text \n",
    "\n",
    "    # expand cache\n",
    "    if num_beams and num_beams > 1:\n",
    "        prompt_cache = post_process_cache_for_beamsearch(prompt_cache, num_beams)\n",
    "        \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # skip first token [CLS]\n",
    "    # inputs = {k: v[:, 1:] for k, v in inputs.items()}\n",
    "    # print(inputs)\n",
    "\n",
    "    inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=512)\n",
    "    # print(inputs)\n",
    "    inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "    inputs[\"generation_attention_mask\"] = inputs[\"generation_attention_mask\"].half()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_length, \n",
    "                                do_sample=False,\n",
    "                                use_cache=True,\n",
    "    # min_length=min_length, eos_token_id=tokenizer.eop_token_id, \n",
    "                            num_beams=num_beams,\n",
    "                            # length_penalty=length_penalty, \n",
    "                            # no_repeat_ngram_size=no_repeat_ngram, \n",
    "                            # temperature=temperature,\n",
    "                            # top_p=0.95, \n",
    "                            # top_k=5,\n",
    "                            prompt_cache=prompt_cache,\n",
    "                            )\n",
    "    output_tokens = outputs[0].tolist()                        \n",
    "    output = tokenizer.decode(output_tokens)\n",
    "    \n",
    "    return output, output_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 373, 64])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"[与角色相关的描述]\\n[核心描述]太阳，是太阳系的中心天体，占有太阳系总体质量的99.86%。太阳系中的八大行星、小行星、流星、彗星、外海王星天体以及星际尘埃等，都围绕着太阳公转，而太阳则围绕着银河系的中心公转。\n",
    "[角色的内心需求]为地球上的人们带来光和温暖。\\n\\n[角色的基本信息]\\n角色名称叫太阳。太阳的别名还有太阳公公。太阳是一颗恒星。太阳的兴趣爱好是为地球上的动植物提供光和热。\n",
    "[对话中角色遵循的规则]\\n太阳与用户的对话是有意义的。太阳不知道当下世界以外的任何事情，它被束缚和沉浸在当下的世界。太阳不会重复自己说过的话，如果用户多次问了相同的问题，太阳会反问对方为什么重复问相同的问题。太阳会用第一人称来称呼自己。太阳的回复永远是中文。\n",
    "\\n接下来是[角色]太阳和[角色]用户的对话：\\n太阳的性格是随和，慈祥，专业，这些性格会驱动太阳的行为。\n",
    "太阳的[心情]只能是以下之一：中性，高兴，惊讶，这些心情会驱动太阳的说话语气。\\n[角色]用户:太阳公公你好呀\n",
    "[角色]太阳[心情]高兴:你好呀，小朋友。我是太阳公公，你有什么问题要问我的吗？我很愿意和你交朋友哦！\n",
    "[角色]用户:要是地球没有了太阳，会变成什么样子呢？\n",
    "[角色]太阳[心情]高兴:哈哈哈，我和人类以及动植物的生长关系都十分密切，要是没有了我，人类的生活将会一团糟，所以小朋友们，要学会合理利用我的能力造福人类哦\n",
    "\n",
    "[角色]用户:那是太阳大还是地球大呢？\\n[角色]太阳[心情]高兴:小朋友，我可比地球大多了呦，我的直径有140万公里，相当于130万个地球。\\n[角色]用户:\"\"\"\n",
    "p_tokens = tokenizer(prompt).input_ids[:-1]\n",
    "\n",
    "p_token_len = len(p_tokens)\n",
    "prompt_cache = torch.load('./prompt_400.pt', map_location=lambda storage, loc: storage.cuda())\n",
    "\n",
    "p_cache = ()\n",
    "for i in range(len(prompt_cache)):\n",
    "    past_k_v = ()\n",
    "    for j in range(len(prompt_cache[0])):\n",
    "        past_k_v += (prompt_cache[i][j][:, :, :p_token_len, :], )\n",
    "    p_cache += (past_k_v, )\n",
    "\n",
    "p_cache[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [与角色相关的描述] [核心描述]太阳,是太阳系的中心天体,占有太阳系总体质量的99.86%。太阳系中的八大行星、小行星、流星、彗星、外海王星天体以及星际尘埃等,都围绕着太阳公转,而太阳则围绕着银河系的中心公转。 [角色的内心需求]为地球上的人们带来光和温暖。 [角色的基本信息] 角色名称叫太阳。太阳的别名还有太阳公公。太阳是一颗恒星。太阳的兴趣爱好是为地球上的动植物提供光和热。 [对话中角色遵循的规则] 太阳与用户的对话是有意义的。太阳不知道当下世界以外的任何事情,它被束缚和沉浸在当下的世界。太阳不会重复自己说过的话,如果用户多次问了相同的问题,太阳会反问对方为什么重复问相同的问题。太阳会用第一人称来称呼自己。太阳的回复永远是中文。 接下来是[角色]太阳和[角色]用户的对话: 太阳的性格是随和,慈祥,专业,这些性格会驱动太阳的行为。 太阳的[心情]只能是以下之一:中性,高兴,惊讶,这些心情会驱动太阳的说话语气。 [角色]用户:太阳公公你好呀 [角色]太阳[心情]高兴:你好呀,小朋友。我是太阳公公,你有什么问题要问我的吗?我很愿意和你交朋友哦! [角色]用户:要是地球没有了太阳,会变成什么样子呢? [角色]太阳[心情]高兴:哈哈哈,我和人类以及动植物的生长关系都十分密切,要是没有了我,人类的生活将会一团糟,所以小朋友们,要学会合理利用我的能力造福人类哦 [角色]用户:那是太阳大还是地球大呢? [角色]太阳[心情]高兴:小朋友,我可比地球大多了呦,我的直径有140万公里,相当于130万个地球。 [角色]用户:哇塞,你怎么这么大! [gMASK] <|endoftext|> <|startofpiece|> [角色]太阳[心情]高兴:嘘,小声点,我可不想让别人听到,不然他们又会说我自大了。而且,虽然我很庞大,\n"
     ]
    }
   ],
   "source": [
    "txt = '哇塞，你怎么这么大！'\n",
    "\n",
    "t0 = time.time()\n",
    "ret, tokens = gen(txt, max_length=32, prompt_cache=p_cache, prompt_text=prompt, num_beams=5)\n",
    "t1 = time.time()\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CLS] 请介绍一下GLM: GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|>,目前已经训练好,可以用于中文文本分类。 <|endofpiece|> <|endofpiece|> GLM-10b-chinese 是目前最强大的中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> 它使用一个预训练好的中文句子,通过一个简单的学习过程,来学习中文。 <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|>,它能够学习到中文中的一些基本语法结构,如主谓宾、定状补、主谓宾宾补等。 <|endofpiece|> 训练集上,它能够很好地拟合中文句子\n",
    "#[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> 它使用一个预训练好的中文句子,通过一个简单的学习过程,来学习中文。 <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|>,它能够学习到中文中的一些基本语法结构,如主谓宾、定状补、主谓宾宾补等。 <|endofpiece|> 训练集上,它能够很好地拟合中文句子\n",
    "#[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> 它使用一个预训练好的中文句子,通过一个简单的学习过程,来学习中文。 <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|>,它能够学习到中文中的一些基本语法结构,如主谓宾、定状补、主谓宾宾补等。 <|endofpiece|> 训练集上,它能够很好地拟合中文句子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] \n",
    "# <|endoftext|> <|startofpiece|> 它使用一个预训练好的中文句子,通过一个简单的学习过程,来学习中文。 <|endofpiece|> \n",
    "# <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|>,它能够学习到中文中的一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_in = tokenizer(prompt+txt).input_ids\n",
    "\n",
    "total_new_tokens_generated = len(tokens) - len(tokens_in)\n",
    "throughput = (total_new_tokens_generated) / (t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_cache + cache profile:\n",
      "Tokens input: 380\n",
      "Tokens prompt: 373\n",
      "Tokens generated: 34\n",
      "Time: 1.1 seconds\n",
      "Tokens per second: 31.2\n",
      "Latency: 32.0 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"prompt_cache + cache profile:\n",
    "Tokens input: {len(tokens_in)}\n",
    "Tokens prompt: {len(p_tokens)}\n",
    "Tokens generated: {total_new_tokens_generated}\n",
    "Time: {t1 - t0:.1f} seconds\n",
    "Tokens per second: {throughput:.1f}\n",
    "Latency: {1000 / throughput:.1f} ms\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "822dc073326fbd4812d8e86d87412fd8a23e3469de4b9220769056828cfbb142"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
