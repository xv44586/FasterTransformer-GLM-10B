{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('./glm_10b_chinese')\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import deepspeed\n",
    "\n",
    "from modeling_glm import GLMForConditionalGeneration\n",
    "from modeling_glm import GLMBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/glm-10b-chinese\", trust_remote_code=True)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"BAAI/glm-10b\", trust_remote_code=True)\n",
    "# model.load_state_dict(torch.load('./blocklm-10b-chinese/mp_rank_00_model_states.pt'),strict=False)\n",
    "# model = GLMForConditionalGeneration.from_pretrained('glm_10b_chinese')\n",
    "model = GLMForConditionalGeneration.from_pretrained('models/character_20230102_1055')\n",
    "model = model.half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-13 10:59:06,654] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\n",
      "[2023-02-13 10:59:06,655] [WARNING] [config_utils.py:67:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2023-02-13 10:59:06,656] [INFO] [logging.py:68:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DeepSpeed-Inference engine\n",
    "ds_engine = deepspeed.init_inference(model,\n",
    "                                 mp_size=1,\n",
    "                                 dtype=torch.half,\n",
    "                                #  checkpoint=None,\n",
    "                                 replace_method='auto',\n",
    "                                 replace_with_kernel_inject=True,\n",
    "                                #  injection_policy={GLMBlock: ('SelfAttention.o', 'DenseReluDense.wo')},\n",
    "                                 enable_cuda_graph=True,\n",
    "                                 \n",
    "                                 )\n",
    "model = ds_engine.module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(text, max_length=32, num_beams=5):\n",
    "    # t0 = time.time()\n",
    "    if '[gMASK]' not in text:\n",
    "        text += '[gMASK]'\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # print(inputs)\n",
    "\n",
    "    inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=512)\n",
    "    inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "    inputs[\"generation_attention_mask\"] = inputs[\"generation_attention_mask\"].half()\n",
    "    # t1 = time.time()\n",
    "    # print(f'tokenizer token use: {t1 - t0:.4f}')\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_length, \n",
    "                                do_sample=False,\n",
    "                                use_cache=True,\n",
    "                            # min_length=min_length, eos_token_id=tokenizer.eop_token_id, \n",
    "                            num_beams=num_beams,\n",
    "                            # length_penalty=length_penalty, \n",
    "                            # no_repeat_ngram_size=no_repeat_ngram, \n",
    "                            # temperature=temperature,\n",
    "                            # top_p=0.95, \n",
    "                            # top_k=5,\n",
    "                            # is_first=True,\n",
    "                            )\n",
    "    output_tokens = outputs[0].tolist()                        \n",
    "    output = tokenizer.decode(output_tokens)\n",
    "    \n",
    "    return output, output_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n",
      "[CLS] [与角色相关的描述] [核心描述]太阳,是太阳系的中心天体,占有太阳系总体质量的99.86%。太阳系中的八大行星、小行星、流星、彗星、外海王星天体以及星际尘埃等,都围绕着太阳公转,而太阳则围绕着银河系的中心公转。 [角色的内心需求]为地球上的人们带来光和温暖。 [角色的基本信息] 角色名称叫太阳。太阳的别名还有太阳公公。太阳是一颗恒星。太阳的兴趣爱好是为地球上的动植物提供光和热。 [对话中角色遵循的规则] 太阳与用户的对话是有意义的。太阳不知道当下世界以外的任何事情,它被束缚和沉浸在当下的世界。太阳不会重复自己说过的话,如果用户多次问了相同的问题,太阳会反问对方为什么重复问相同的问题。太阳会用第一人称来称呼自己。太阳的回复永远是中文。 接下来是[角色]太阳和[角色]用户的对话: 太阳的性格是随和,慈祥,专业,这些性格会驱动太阳的行为。 太阳的[心情]只能是以下之一:中性,高兴,惊讶,这些心情会驱动太阳的说话语气。 [角色]用户:太阳公公你好呀 [角色]太阳[心情]高兴:你好呀,小朋友。我是太阳公公,你有什么问题要问我的吗?我很愿意和你交朋友哦! [角色]用户:要是地球没有了太阳,会变成什么样子呢? [角色]太阳[心情]高兴:哈哈哈,我和人类以及动植物的生长关系都十分密切,要是没有了我,人类的生活将会一团糟,所以小朋友们,要学会合理利用我的能力造福人类哦 [角色]用户:那是太阳大还是地球大呢? [角色]太阳[心情]高兴:小朋友,我可比地球大多了呦,我的直径有140万公里,相当于130万个地球。 [角色]用户:哇塞,你怎么这么大! [gMASK] <|endoftext|> <|startofpiece|> [角色]太阳[心情]高兴:那是因为我燃烧氢气,然后膨胀形成的。而且我在不断运动中,每小时要绕银河系中心转10圈。\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import time\n",
    "prompt = \"\"\"[与角色相关的描述]\\n[核心描述]太阳，是太阳系的中心天体，占有太阳系总体质量的99.86%。太阳系中的八大行星、小行星、流星、彗星、外海王星天体以及星际尘埃等，都围绕着太阳公转，而太阳则围绕着银河系的中心公转。\n",
    "[角色的内心需求]为地球上的人们带来光和温暖。\\n\\n[角色的基本信息]\\n角色名称叫太阳。太阳的别名还有太阳公公。太阳是一颗恒星。太阳的兴趣爱好是为地球上的动植物提供光和热。\n",
    "[对话中角色遵循的规则]\\n太阳与用户的对话是有意义的。太阳不知道当下世界以外的任何事情，它被束缚和沉浸在当下的世界。太阳不会重复自己说过的话，如果用户多次问了相同的问题，太阳会反问对方为什么重复问相同的问题。太阳会用第一人称来称呼自己。太阳的回复永远是中文。\n",
    "\\n接下来是[角色]太阳和[角色]用户的对话：\\n太阳的性格是随和，慈祥，专业，这些性格会驱动太阳的行为。\n",
    "太阳的[心情]只能是以下之一：中性，高兴，惊讶，这些心情会驱动太阳的说话语气。\\n[角色]用户:太阳公公你好呀\n",
    "[角色]太阳[心情]高兴:你好呀，小朋友。我是太阳公公，你有什么问题要问我的吗？我很愿意和你交朋友哦！\n",
    "[角色]用户:要是地球没有了太阳，会变成什么样子呢？\n",
    "[角色]太阳[心情]高兴:哈哈哈，我和人类以及动植物的生长关系都十分密切，要是没有了我，人类的生活将会一团糟，所以小朋友们，要学会合理利用我的能力造福人类哦\n",
    "\n",
    "[角色]用户:那是太阳大还是地球大呢？\\n[角色]太阳[心情]高兴:小朋友，我可比地球大多了呦，我的直径有140万公里，相当于130万个地球。\\n[角色]用户:\"\"\"\n",
    "\n",
    "txt = '哇塞，你怎么这么大！'\n",
    "p_tokens = tokenizer(prompt).input_ids\n",
    "print(len(p_tokens))\n",
    "\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "ret, tokens = gen(prompt + txt, max_length=32, num_beams=None)\n",
    "t1 = time.time()\n",
    "print(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> 它使用一个预训练好的中文句子,通过一个简单的学习过程,来学习中文。 <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|>,它能够学习到中文中的一些基本语法结构,如主谓宾、定状补、主谓宾宾补等。 <|endofpiece|> 训练集上,它能够很好地拟合中文句子\n",
    "#[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> 它使用一个预训练好的中文句子,通过一个简单的学习过程,来学习中文。 <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|> <|endofpiece|>,它能够学习到中文中的一些基本语法结构,如主谓宾、定状补、主谓宾宾补等。 <|endofpiece|> 训练集上,它能够很好地拟合中文句子\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> <|endofpiece|>, GLM-10b-Chinese 模型在 C++ 代码上运行,它使用一个预训练好的 C++ 类来代表中文。\n",
    "\n",
    "#[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预\n",
    "#[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预\n",
    "#[CLS] GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。 [gMASK] <|endoftext|> <|startofpiece|> GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预训练语言模型。GLM-10b-chinese 是一个10B的中文预\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_in = tokenizer(prompt+txt).input_ids\n",
    "\n",
    "total_new_tokens_generated = len(tokens) - len(tokens_in)\n",
    "throughput = (total_new_tokens_generated) / (t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepspeed inference profile:\n",
      "Tokens input: 380\n",
      "Tokens prompt: 374\n",
      "Tokens generated: 34\n",
      "Time: 1.0 seconds\n",
      "Tokens per second: 35.1\n",
      "Latency: 28.5 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Deepspeed inference profile:\n",
    "Tokens input: {len(tokens_in)}\n",
    "Tokens prompt: {len(p_tokens)}\n",
    "Tokens generated: {total_new_tokens_generated}\n",
    "Time: {t1 - t0:.1f} seconds\n",
    "Tokens per second: {throughput:.1f}\n",
    "Latency: {1000 / throughput:.1f} ms\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "822dc073326fbd4812d8e86d87412fd8a23e3469de4b9220769056828cfbb142"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
